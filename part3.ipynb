{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNL+ghP5TFK08RqYnDBl3QJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilia1221/test/blob/main/part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rwZzJknNfnq",
        "outputId": "77e00db9-459e-450b-82c5-32001f74ed23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W0TfECxNnY9"
      },
      "source": [
        "!cp '/content/drive/My Drive/test_task/dataset-v2.zip' dataset-v2.zip\n",
        "!unzip -q dataset-v2.zip\n",
        "# !cp '/content/drive/My Drive/test_task/wavefront.py' wavefront.py\n",
        "! wget https://raw.githubusercontent.com/ilia1221/test/main/wavefront.py -O wavefront.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbedcWN5OHfW"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3X8wID6axFA"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZKdpuBw93mg"
      },
      "source": [
        "Класс с параметрами обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miQLauCkr7Gl"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  exp_name: str\n",
        "  batch_size: int\n",
        "  test_batch_size: int\n",
        "  epochs: int\n",
        "  use_sgd: bool\n",
        "  lr: float\n",
        "  momentum: float\n",
        "  scheduler: str\n",
        "  seed: int  \n",
        "  num_points: int\n",
        "  dropout: float\n",
        "  emb_dims: int\n",
        "  k: int\n",
        "  device: str\n",
        "  checkpoints_path: str"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEhTWqDu-BZO"
      },
      "source": [
        "Задаем архитектуру сети DGCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B49dis6_U9n"
      },
      "source": [
        "def knn(x, k):\n",
        "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
        "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
        "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
        " \n",
        "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
        "    return idx\n",
        "\n",
        "\n",
        "def get_graph_feature(x, k=20, idx=None, dim9=False):\n",
        "    batch_size = x.size(0)\n",
        "    num_points = x.size(2)\n",
        "    x = x.view(batch_size, -1, num_points)\n",
        "    if idx is None:\n",
        "        if dim9 == False:\n",
        "            idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
        "        else:\n",
        "            idx = knn(x[:, 6:], k=k)    \n",
        "\n",
        "    idx_base = torch.arange(0, batch_size, device=x.device).view(-1, 1, 1)*num_points\n",
        "\n",
        "    idx = idx + idx_base\n",
        "\n",
        "    idx = idx.view(-1)\n",
        " \n",
        "    _, num_dims, _ = x.size()\n",
        "\n",
        "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
        "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
        "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
        "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
        "    \n",
        "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
        "  \n",
        "    return feature      # (batch_size, 2*num_dims, num_points, k)\n",
        "\n",
        "\n",
        "class DGCNN_cls(nn.Module):\n",
        "    def __init__(self, args, output_channels=40):\n",
        "        super(DGCNN_cls, self).__init__()\n",
        "        self.args = args\n",
        "        self.k = args.k\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.bn5 = nn.BatchNorm1d(args.emb_dims)\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
        "                                   self.bn1,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
        "                                   self.bn2,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
        "                                   self.bn3,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
        "                                   self.bn4,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv5 = nn.Sequential(nn.Conv1d(512, args.emb_dims, kernel_size=1, bias=False),\n",
        "                                   self.bn5,\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.linear1 = nn.Linear(args.emb_dims*2, 512, bias=False)\n",
        "        self.bn6 = nn.BatchNorm1d(512)\n",
        "        self.dp1 = nn.Dropout(p=args.dropout)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        self.dp2 = nn.Dropout(p=args.dropout)\n",
        "        self.linear3 = nn.Linear(256, output_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = get_graph_feature(x, k=self.k)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n",
        "        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
        "        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
        "\n",
        "        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
        "        x = self.conv2(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
        "        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
        "\n",
        "        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
        "        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n",
        "        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n",
        "\n",
        "        x = get_graph_feature(x3, k=self.k)     # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n",
        "        x = self.conv4(x)                       # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n",
        "        x4 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n",
        "\n",
        "        x = torch.cat((x1, x2, x3, x4), dim=1)  # (batch_size, 64+64+128+256, num_points)\n",
        "\n",
        "        x = self.conv5(x)                       # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n",
        "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
        "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n",
        "        x = torch.cat((x1, x2), 1)              # (batch_size, emb_dims*2)\n",
        "\n",
        "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2) # (batch_size, emb_dims*2) -> (batch_size, 512)\n",
        "        x = self.dp1(x)\n",
        "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2) # (batch_size, 512) -> (batch_size, 256)\n",
        "        x = self.dp2(x)\n",
        "        x = self.linear3(x)                                             # (batch_size, 256) -> (batch_size, output_channels)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8_R3PvK-LN_"
      },
      "source": [
        "Класс и функции для работы с датасетом\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSOfwwjBOwoN"
      },
      "source": [
        "from wavefront import load_obj\n",
        "\n",
        "def translate_pointcloud(pointcloud):\n",
        "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
        "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
        "       \n",
        "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
        "    return translated_pointcloud\n",
        "\n",
        "\n",
        "def jitter_pointcloud(pointcloud, sigma=0.01, clip=0.02):\n",
        "    N, C = pointcloud.shape\n",
        "    pointcloud += np.clip(sigma * np.random.randn(N, C), -1*clip, clip)\n",
        "    return pointcloud\n",
        "\n",
        "\n",
        "def rotate_pointcloud(pointcloud):\n",
        "    theta = np.pi*2 * np.random.uniform()\n",
        "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
        "    pointcloud[:,[0,2]] = pointcloud[:,[0,2]].dot(rotation_matrix) # random rotation (x,z)\n",
        "    return pointcloud\n",
        "\n",
        "\n",
        "label2num = {\n",
        "    'cone': 0,\n",
        "    'cube': 1,\n",
        "    'cylinder': 2,\n",
        "    'plane': 3,\n",
        "    'torus': 4,\n",
        "    'uv_sphere': 5,\n",
        "}\n",
        "\n",
        "num2label = {v: k for k, v in label2num.items()}\n",
        "\n",
        "def load_data(path, partition):\n",
        "  data = []\n",
        "  labels = []\n",
        "  \n",
        "  classes = os.listdir(path)\n",
        "  for cls in classes:\n",
        "    dir_path = os.path.join(path, cls, partition)\n",
        "    files = [os.path.join(dir_path, f) for f in os.listdir(dir_path)]    \n",
        "\n",
        "    for f in files:\n",
        "      wavefront_obj = load_obj(f)\n",
        "      data.append(np.asarray(wavefront_obj.vertices, dtype=np.float32))      \n",
        "      labels.append(label2num[cls])\n",
        "    \n",
        "  return data, labels\n",
        "\n",
        "\n",
        "class GeometricShapesDataset(Dataset):\n",
        "  def __init__(self, path, num_points, partition='train'):      \n",
        "      self.num_points = num_points\n",
        "      self.partition = partition    \n",
        "      self.data, self.label = load_data(path, self.partition) \n",
        "      print(f'Dataset size: {len(self.data)}')     \n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      pointcloud = self.data[idx]      \n",
        "      label = self.label[idx]\n",
        "\n",
        "      if self.partition == 'train':\n",
        "         pointcloud = translate_pointcloud(pointcloud)\n",
        "         np.random.shuffle(pointcloud)\n",
        "\n",
        "      if pointcloud.shape[0] < self.num_points:\n",
        "         pad_to = self.num_points - pointcloud.shape[0]\n",
        "         pointcloud = np.pad(pointcloud, [(0, pad_to), (0, 0)], mode='constant')\n",
        "\n",
        "      return pointcloud, label\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuX-WzC_-WJ4"
      },
      "source": [
        "Train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19E2JLQb_eKO"
      },
      "source": [
        "def train(args):\n",
        "    train_dataset = GeometricShapesDataset('dataset-v2', 1024, partition='train')\n",
        "    test_dataset = GeometricShapesDataset('dataset-v2', 1024, partition='test')\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              num_workers=8,\n",
        "                              batch_size=args.batch_size,\n",
        "                              shuffle=True,\n",
        "                              drop_last=True)\n",
        "    \n",
        "    test_loader = DataLoader(test_dataset,\n",
        "                             num_workers=8,\n",
        "                             batch_size=args.test_batch_size,\n",
        "                             shuffle=True,\n",
        "                             drop_last=False)\n",
        "\n",
        "    device = args.device\n",
        "    model = DGCNN_cls(args, output_channels=len(label2num)).to(device)    \n",
        "\n",
        "    if args.use_sgd:        \n",
        "        opt = optim.SGD(model.parameters(), lr=args.lr*100, momentum=args.momentum, weight_decay=1e-4)\n",
        "    else:        \n",
        "        opt = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "\n",
        "    if args.scheduler == 'cos':\n",
        "        scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=1e-3)\n",
        "    elif args.scheduler == 'step':\n",
        "        scheduler = StepLR(opt, step_size=20, gamma=0.7)\n",
        "    \n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_test_acc = 0\n",
        "    for epoch in range(args.epochs):\n",
        "        outstr = f'Epoch: {epoch} lr: {opt.param_groups[0][\"lr\"]:.3e}'\n",
        "        ####################\n",
        "        # Train\n",
        "        ####################\n",
        "        train_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.train()\n",
        "        train_pred = []\n",
        "        train_true = []\n",
        "        for data, label in train_loader:\n",
        "            data, label = data.to(device), label.to(device).squeeze()\n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            opt.zero_grad()\n",
        "            logits = model(data)\n",
        "            loss = criterion(logits, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            preds = logits.max(dim=1)[1]\n",
        "            count += batch_size\n",
        "            train_loss += loss.item() * batch_size\n",
        "            train_true.append(label.cpu().numpy())\n",
        "            train_pred.append(preds.detach().cpu().numpy())\n",
        "        if args.scheduler == 'cos':\n",
        "            scheduler.step()\n",
        "        elif args.scheduler == 'step':\n",
        "            if opt.param_groups[0]['lr'] > 1e-5:\n",
        "                scheduler.step()\n",
        "            if opt.param_groups[0]['lr'] < 1e-5:\n",
        "                for param_group in opt.param_groups:\n",
        "                    param_group['lr'] = 1e-5\n",
        "\n",
        "        train_true = np.concatenate(train_true)\n",
        "        train_pred = np.concatenate(train_pred)\n",
        "        outstr += '   train_loss: %.6f, train_acc: %.6f, train_avg_acc: %.6f' % ( train_loss*1.0/count,\n",
        "                                                                                 metrics.accuracy_score(\n",
        "                                                                                     train_true, train_pred),\n",
        "                                                                                 metrics.balanced_accuracy_score(\n",
        "                                                                                     train_true, train_pred))        \n",
        "                \n",
        "        ####################\n",
        "        # Test\n",
        "        ####################\n",
        "        test_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.eval()\n",
        "        test_pred = []\n",
        "        test_true = []\n",
        "        with torch.no_grad():\n",
        "          for data, label in test_loader:\n",
        "              data, label = data.to(device), label.to(device).squeeze()\n",
        "              data = data.permute(0, 2, 1)\n",
        "              batch_size = data.size()[0]\n",
        "              logits = model(data)\n",
        "              loss = criterion(logits, label)\n",
        "              preds = logits.max(dim=1)[1]\n",
        "              count += batch_size\n",
        "              test_loss += loss.item() * batch_size\n",
        "              test_true.append(label.cpu().numpy())\n",
        "              test_pred.append(preds.detach().cpu().numpy())\n",
        "          test_true = np.concatenate(test_true)\n",
        "          test_pred = np.concatenate(test_pred)\n",
        "          test_acc = metrics.accuracy_score(test_true, test_pred)\n",
        "          avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
        "          outstr += '   test_loss: %.6f, test_acc: %.6f, test_avg_acc: %.6f' % ( test_loss*1.0/count,\n",
        "                                                                                test_acc,\n",
        "                                                                                avg_per_class_acc)\n",
        "        \n",
        "        print(outstr)\n",
        "        if test_acc >= best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            torch.save(model.state_dict(), os.path.join(args.checkpoints_path, 'model_best.pth'))\n",
        "\n",
        "\n",
        "def test(args):    \n",
        "    dataset = GeometricShapesDataset('dataset-v2', 1024, partition='valid')\n",
        "    loader = DataLoader(dataset,\n",
        "                              num_workers=8,\n",
        "                              batch_size=args.batch_size,\n",
        "                              shuffle=False,\n",
        "                              drop_last=False)\n",
        "\n",
        "    device = args.device\n",
        "\n",
        "    model = DGCNN_cls(args).to(device)    \n",
        "    model.load_state_dict(torch.load(os.path.join(args.checkpoints_path, 'model_best.pth')))\n",
        "    model.eval()\n",
        "    test_acc = 0.0\n",
        "    count = 0.0\n",
        "    test_true = []\n",
        "    test_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data, label in loader:\n",
        "          data, label = data.to(device), label.to(device).squeeze()\n",
        "          data = data.permute(0, 2, 1)\n",
        "          batch_size = data.size()[0]\n",
        "          logits = model(data)\n",
        "          preds = logits.max(dim=1)[1]\n",
        "          test_true.append(label.cpu().numpy())\n",
        "          test_pred.append(preds.detach().cpu().numpy())\n",
        "\n",
        "    test_true = np.concatenate(test_true)\n",
        "    test_pred = np.concatenate(test_pred)\n",
        "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
        "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
        "    \n",
        "    report = classification_report(test_true, test_pred, target_names=list(label2num.keys()))\n",
        "    print(report)\n",
        "\n",
        "    outstr = 'Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc)\n",
        "    print(outstr)\n",
        "\n",
        "\n",
        "def convert_to_jit(args):\n",
        "    model = DGCNN_cls(args, output_channels=len(label2num)).to(args.device)\n",
        "    model.load_state_dict(torch.load(os.path.join(args.checkpoints_path, 'model_best.pth')))\n",
        "    model.eval()\n",
        "    print(model)\n",
        "\n",
        "    x = torch.rand(3, 1024).unsqueeze(0).to(args.device)\n",
        "    traced = torch.jit.trace(model, x)\n",
        "\n",
        "    torch.jit.save(traced, os.path.join(args.checkpoints_path, f'model_best_{args.device}.pt'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnCQHjSh-eou"
      },
      "source": [
        "Run train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AW30krEkrw22",
        "outputId": "f9c8bf94-9709-4d08-b168-81349c459f92"
      },
      "source": [
        "config = Config(\n",
        "  exp_name = 'exp',\n",
        "  batch_size = 32,\n",
        "  test_batch_size = 16,\n",
        "  epochs = 250,\n",
        "  use_sgd = True,\n",
        "  lr = 0.001,\n",
        "  momentum = 0.9,\n",
        "  scheduler = 'cos',\n",
        "  seed = 1,     \n",
        "  num_points = 1024,\n",
        "  dropout = 0.5,\n",
        "  emb_dims = 1024,\n",
        "  k = 20,\n",
        "  device = 'cuda',\n",
        "  checkpoints_path = '/content/drive/My Drive/test_task_2/checkpoints/',\n",
        ") \n",
        "\n",
        "torch.manual_seed(config.seed)    \n",
        "torch.cuda.manual_seed(config.seed)    \n",
        "\n",
        "train(config)    \n",
        "\n",
        "# test(config)\n",
        "\n",
        "# convert_to_jit(config)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 1200\n",
            "Dataset size: 600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 lr: 1.000e-01   train_loss: 1.099090, train_acc: 0.683277, train_avg_acc: 0.554430   test_loss: 1.540802, test_acc: 0.580000, test_avg_acc: 0.580000\n",
            "Epoch: 1 lr: 1.000e-01   train_loss: 0.962094, train_acc: 0.721284, train_avg_acc: 0.579952   test_loss: 0.626603, test_acc: 0.723333, test_avg_acc: 0.723333\n",
            "Epoch: 2 lr: 9.998e-02   train_loss: 0.545507, train_acc: 0.776182, train_avg_acc: 0.640210   test_loss: 0.558361, test_acc: 0.735000, test_avg_acc: 0.735000\n",
            "Epoch: 3 lr: 9.996e-02   train_loss: 0.451606, train_acc: 0.804054, train_avg_acc: 0.670563   test_loss: 0.583768, test_acc: 0.708333, test_avg_acc: 0.708333\n",
            "Epoch: 4 lr: 9.994e-02   train_loss: 0.413898, train_acc: 0.810811, train_avg_acc: 0.670175   test_loss: 0.567091, test_acc: 0.698333, test_avg_acc: 0.698333\n",
            "Epoch: 5 lr: 9.990e-02   train_loss: 0.396840, train_acc: 0.826858, train_avg_acc: 0.701873   test_loss: 0.531916, test_acc: 0.720000, test_avg_acc: 0.720000\n",
            "Epoch: 6 lr: 9.986e-02   train_loss: 0.417963, train_acc: 0.816723, train_avg_acc: 0.691289   test_loss: 0.481741, test_acc: 0.758333, test_avg_acc: 0.758333\n",
            "Epoch: 7 lr: 9.981e-02   train_loss: 0.396211, train_acc: 0.815034, train_avg_acc: 0.677730   test_loss: 0.473533, test_acc: 0.758333, test_avg_acc: 0.758333\n",
            "Epoch: 8 lr: 9.975e-02   train_loss: 0.356749, train_acc: 0.832770, train_avg_acc: 0.700091   test_loss: 0.510287, test_acc: 0.743333, test_avg_acc: 0.743333\n",
            "Epoch: 9 lr: 9.968e-02   train_loss: 0.390870, train_acc: 0.826858, train_avg_acc: 0.701791   test_loss: 0.519427, test_acc: 0.731667, test_avg_acc: 0.731667\n",
            "Epoch: 10 lr: 9.961e-02   train_loss: 0.327168, train_acc: 0.853885, train_avg_acc: 0.726295   test_loss: 0.432200, test_acc: 0.781667, test_avg_acc: 0.781667\n",
            "Epoch: 11 lr: 9.953e-02   train_loss: 0.302798, train_acc: 0.859797, train_avg_acc: 0.748084   test_loss: 0.401902, test_acc: 0.805000, test_avg_acc: 0.805000\n",
            "Epoch: 12 lr: 9.944e-02   train_loss: 0.311207, train_acc: 0.851351, train_avg_acc: 0.746051   test_loss: 0.411330, test_acc: 0.791667, test_avg_acc: 0.791667\n",
            "Epoch: 13 lr: 9.934e-02   train_loss: 0.303692, train_acc: 0.858953, train_avg_acc: 0.760141   test_loss: 0.427539, test_acc: 0.771667, test_avg_acc: 0.771667\n",
            "Epoch: 14 lr: 9.924e-02   train_loss: 0.312978, train_acc: 0.850507, train_avg_acc: 0.734459   test_loss: 0.467898, test_acc: 0.781667, test_avg_acc: 0.781667\n",
            "Epoch: 15 lr: 9.912e-02   train_loss: 0.283433, train_acc: 0.863176, train_avg_acc: 0.763321   test_loss: 0.373843, test_acc: 0.826667, test_avg_acc: 0.826667\n",
            "Epoch: 16 lr: 9.900e-02   train_loss: 0.296811, train_acc: 0.867399, train_avg_acc: 0.765290   test_loss: 0.500718, test_acc: 0.741667, test_avg_acc: 0.741667\n",
            "Epoch: 17 lr: 9.887e-02   train_loss: 0.281474, train_acc: 0.859797, train_avg_acc: 0.762796   test_loss: 0.482297, test_acc: 0.720000, test_avg_acc: 0.720000\n",
            "Epoch: 18 lr: 9.874e-02   train_loss: 0.322069, train_acc: 0.849662, train_avg_acc: 0.759294   test_loss: 0.527586, test_acc: 0.710000, test_avg_acc: 0.710000\n",
            "Epoch: 19 lr: 9.860e-02   train_loss: 0.333742, train_acc: 0.853885, train_avg_acc: 0.733584   test_loss: 0.442603, test_acc: 0.801667, test_avg_acc: 0.801667\n",
            "Epoch: 20 lr: 9.844e-02   train_loss: 0.298163, train_acc: 0.848818, train_avg_acc: 0.743542   test_loss: 0.420116, test_acc: 0.778333, test_avg_acc: 0.778333\n",
            "Epoch: 21 lr: 9.829e-02   train_loss: 0.286001, train_acc: 0.857264, train_avg_acc: 0.746092   test_loss: 0.397889, test_acc: 0.811667, test_avg_acc: 0.811667\n",
            "Epoch: 22 lr: 9.812e-02   train_loss: 0.297145, train_acc: 0.848818, train_avg_acc: 0.739601   test_loss: 0.417904, test_acc: 0.801667, test_avg_acc: 0.801667\n",
            "Epoch: 23 lr: 9.795e-02   train_loss: 0.281219, train_acc: 0.861486, train_avg_acc: 0.777618   test_loss: 0.420705, test_acc: 0.770000, test_avg_acc: 0.770000\n",
            "Epoch: 24 lr: 9.777e-02   train_loss: 0.274512, train_acc: 0.862331, train_avg_acc: 0.758153   test_loss: 0.464570, test_acc: 0.733333, test_avg_acc: 0.733333\n",
            "Epoch: 25 lr: 9.758e-02   train_loss: 0.261008, train_acc: 0.864020, train_avg_acc: 0.776398   test_loss: 0.411077, test_acc: 0.806667, test_avg_acc: 0.806667\n",
            "Epoch: 26 lr: 9.738e-02   train_loss: 0.294031, train_acc: 0.862331, train_avg_acc: 0.767993   test_loss: 0.452521, test_acc: 0.773333, test_avg_acc: 0.773333\n",
            "Epoch: 27 lr: 9.718e-02   train_loss: 0.307925, train_acc: 0.853041, train_avg_acc: 0.762205   test_loss: 0.551191, test_acc: 0.720000, test_avg_acc: 0.720000\n",
            "Epoch: 28 lr: 9.697e-02   train_loss: 0.275805, train_acc: 0.861486, train_avg_acc: 0.775044   test_loss: 0.398202, test_acc: 0.836667, test_avg_acc: 0.836667\n",
            "Epoch: 29 lr: 9.675e-02   train_loss: 0.267568, train_acc: 0.869932, train_avg_acc: 0.776068   test_loss: 0.389472, test_acc: 0.813333, test_avg_acc: 0.813333\n",
            "Epoch: 30 lr: 9.652e-02   train_loss: 0.288909, train_acc: 0.848818, train_avg_acc: 0.750705   test_loss: 0.410104, test_acc: 0.790000, test_avg_acc: 0.790000\n",
            "Epoch: 31 lr: 9.629e-02   train_loss: 0.251984, train_acc: 0.878378, train_avg_acc: 0.781523   test_loss: 0.412842, test_acc: 0.800000, test_avg_acc: 0.800000\n",
            "Epoch: 32 lr: 9.605e-02   train_loss: 0.277894, train_acc: 0.862331, train_avg_acc: 0.771146   test_loss: 0.380694, test_acc: 0.800000, test_avg_acc: 0.800000\n",
            "Epoch: 33 lr: 9.580e-02   train_loss: 0.251695, train_acc: 0.875000, train_avg_acc: 0.803767   test_loss: 0.401213, test_acc: 0.776667, test_avg_acc: 0.776667\n",
            "Epoch: 34 lr: 9.555e-02   train_loss: 0.243530, train_acc: 0.874155, train_avg_acc: 0.804270   test_loss: 0.395791, test_acc: 0.790000, test_avg_acc: 0.790000\n",
            "Epoch: 35 lr: 9.529e-02   train_loss: 0.274667, train_acc: 0.869932, train_avg_acc: 0.789551   test_loss: 0.370353, test_acc: 0.868333, test_avg_acc: 0.868333\n",
            "Epoch: 36 lr: 9.502e-02   train_loss: 0.273131, train_acc: 0.864020, train_avg_acc: 0.772723   test_loss: 0.362004, test_acc: 0.808333, test_avg_acc: 0.808333\n",
            "Epoch: 37 lr: 9.475e-02   train_loss: 0.298021, train_acc: 0.859797, train_avg_acc: 0.771701   test_loss: 0.459633, test_acc: 0.771667, test_avg_acc: 0.771667\n",
            "Epoch: 38 lr: 9.446e-02   train_loss: 0.276181, train_acc: 0.875845, train_avg_acc: 0.799904   test_loss: 0.415284, test_acc: 0.775000, test_avg_acc: 0.775000\n",
            "Epoch: 39 lr: 9.417e-02   train_loss: 0.261740, train_acc: 0.868243, train_avg_acc: 0.777026   test_loss: 0.374970, test_acc: 0.826667, test_avg_acc: 0.826667\n",
            "Epoch: 40 lr: 9.388e-02   train_loss: 0.250599, train_acc: 0.870777, train_avg_acc: 0.783116   test_loss: 0.356324, test_acc: 0.846667, test_avg_acc: 0.846667\n",
            "Epoch: 41 lr: 9.357e-02   train_loss: 0.239618, train_acc: 0.877534, train_avg_acc: 0.803113   test_loss: 0.377360, test_acc: 0.825000, test_avg_acc: 0.825000\n",
            "Epoch: 42 lr: 9.326e-02   train_loss: 0.250668, train_acc: 0.880068, train_avg_acc: 0.803504   test_loss: 0.395771, test_acc: 0.818333, test_avg_acc: 0.818333\n",
            "Epoch: 43 lr: 9.295e-02   train_loss: 0.267561, train_acc: 0.873311, train_avg_acc: 0.805383   test_loss: 0.385854, test_acc: 0.806667, test_avg_acc: 0.806667\n",
            "Epoch: 44 lr: 9.262e-02   train_loss: 0.261875, train_acc: 0.884291, train_avg_acc: 0.822219   test_loss: 0.456411, test_acc: 0.736667, test_avg_acc: 0.736667\n",
            "Epoch: 45 lr: 9.229e-02   train_loss: 0.229141, train_acc: 0.883446, train_avg_acc: 0.794186   test_loss: 0.335977, test_acc: 0.846667, test_avg_acc: 0.846667\n",
            "Epoch: 46 lr: 9.196e-02   train_loss: 0.242155, train_acc: 0.875845, train_avg_acc: 0.803454   test_loss: 0.334641, test_acc: 0.861667, test_avg_acc: 0.861667\n",
            "Epoch: 47 lr: 9.161e-02   train_loss: 0.239075, train_acc: 0.879223, train_avg_acc: 0.818586   test_loss: 0.444064, test_acc: 0.773333, test_avg_acc: 0.773333\n",
            "Epoch: 48 lr: 9.126e-02   train_loss: 0.236087, train_acc: 0.880068, train_avg_acc: 0.823669   test_loss: 0.413250, test_acc: 0.831667, test_avg_acc: 0.831667\n",
            "Epoch: 49 lr: 9.091e-02   train_loss: 0.249982, train_acc: 0.885135, train_avg_acc: 0.811017   test_loss: 0.446532, test_acc: 0.798333, test_avg_acc: 0.798333\n",
            "Epoch: 50 lr: 9.055e-02   train_loss: 0.258946, train_acc: 0.861486, train_avg_acc: 0.788569   test_loss: 0.429725, test_acc: 0.776667, test_avg_acc: 0.776667\n",
            "Epoch: 51 lr: 9.018e-02   train_loss: 0.260741, train_acc: 0.876689, train_avg_acc: 0.808998   test_loss: 0.421874, test_acc: 0.768333, test_avg_acc: 0.768333\n",
            "Epoch: 52 lr: 8.980e-02   train_loss: 0.229571, train_acc: 0.885980, train_avg_acc: 0.817105   test_loss: 0.373944, test_acc: 0.810000, test_avg_acc: 0.810000\n",
            "Epoch: 53 lr: 8.942e-02   train_loss: 0.224113, train_acc: 0.897804, train_avg_acc: 0.831611   test_loss: 0.338386, test_acc: 0.841667, test_avg_acc: 0.841667\n",
            "Epoch: 54 lr: 8.903e-02   train_loss: 0.205446, train_acc: 0.902872, train_avg_acc: 0.864462   test_loss: 0.361999, test_acc: 0.825000, test_avg_acc: 0.825000\n",
            "Epoch: 55 lr: 8.864e-02   train_loss: 0.217291, train_acc: 0.893581, train_avg_acc: 0.828737   test_loss: 0.366137, test_acc: 0.828333, test_avg_acc: 0.828333\n",
            "Epoch: 56 lr: 8.824e-02   train_loss: 0.220393, train_acc: 0.895270, train_avg_acc: 0.829286   test_loss: 0.298163, test_acc: 0.866667, test_avg_acc: 0.866667\n",
            "Epoch: 57 lr: 8.784e-02   train_loss: 0.226485, train_acc: 0.902027, train_avg_acc: 0.857838   test_loss: 0.350802, test_acc: 0.830000, test_avg_acc: 0.830000\n",
            "Epoch: 58 lr: 8.742e-02   train_loss: 0.223897, train_acc: 0.889358, train_avg_acc: 0.830186   test_loss: 0.314743, test_acc: 0.866667, test_avg_acc: 0.866667\n",
            "Epoch: 59 lr: 8.701e-02   train_loss: 0.218164, train_acc: 0.908784, train_avg_acc: 0.851096   test_loss: 0.327851, test_acc: 0.858333, test_avg_acc: 0.858333\n",
            "Epoch: 60 lr: 8.658e-02   train_loss: 0.214839, train_acc: 0.902872, train_avg_acc: 0.849747   test_loss: 0.301610, test_acc: 0.885000, test_avg_acc: 0.885000\n",
            "Epoch: 61 lr: 8.616e-02   train_loss: 0.225425, train_acc: 0.896115, train_avg_acc: 0.848030   test_loss: 0.316720, test_acc: 0.873333, test_avg_acc: 0.873333\n",
            "Epoch: 62 lr: 8.572e-02   train_loss: 0.235231, train_acc: 0.880912, train_avg_acc: 0.807822   test_loss: 0.393381, test_acc: 0.803333, test_avg_acc: 0.803333\n",
            "Epoch: 63 lr: 8.528e-02   train_loss: 0.211828, train_acc: 0.907939, train_avg_acc: 0.856495   test_loss: 0.336344, test_acc: 0.843333, test_avg_acc: 0.843333\n",
            "Epoch: 64 lr: 8.484e-02   train_loss: 0.199120, train_acc: 0.908784, train_avg_acc: 0.848570   test_loss: 0.274782, test_acc: 0.890000, test_avg_acc: 0.890000\n",
            "Epoch: 65 lr: 8.439e-02   train_loss: 0.188490, train_acc: 0.909628, train_avg_acc: 0.872746   test_loss: 0.295314, test_acc: 0.866667, test_avg_acc: 0.866667\n",
            "Epoch: 66 lr: 8.393e-02   train_loss: 0.192706, train_acc: 0.917230, train_avg_acc: 0.870432   test_loss: 0.321162, test_acc: 0.860000, test_avg_acc: 0.860000\n",
            "Epoch: 67 lr: 8.347e-02   train_loss: 0.200662, train_acc: 0.913007, train_avg_acc: 0.869638   test_loss: 0.310162, test_acc: 0.896667, test_avg_acc: 0.896667\n",
            "Epoch: 68 lr: 8.300e-02   train_loss: 0.202571, train_acc: 0.901182, train_avg_acc: 0.836141   test_loss: 0.273923, test_acc: 0.873333, test_avg_acc: 0.873333\n",
            "Epoch: 69 lr: 8.253e-02   train_loss: 0.188681, train_acc: 0.919764, train_avg_acc: 0.882446   test_loss: 0.376583, test_acc: 0.821667, test_avg_acc: 0.821667\n",
            "Epoch: 70 lr: 8.205e-02   train_loss: 0.230256, train_acc: 0.896959, train_avg_acc: 0.838024   test_loss: 0.300578, test_acc: 0.881667, test_avg_acc: 0.881667\n",
            "Epoch: 71 lr: 8.157e-02   train_loss: 0.185154, train_acc: 0.919764, train_avg_acc: 0.888520   test_loss: 0.313789, test_acc: 0.863333, test_avg_acc: 0.863333\n",
            "Epoch: 72 lr: 8.108e-02   train_loss: 0.209743, train_acc: 0.914696, train_avg_acc: 0.866892   test_loss: 0.329233, test_acc: 0.873333, test_avg_acc: 0.873333\n",
            "Epoch: 73 lr: 8.059e-02   train_loss: 0.203315, train_acc: 0.907939, train_avg_acc: 0.861489   test_loss: 0.330285, test_acc: 0.840000, test_avg_acc: 0.840000\n",
            "Epoch: 74 lr: 8.010e-02   train_loss: 0.181460, train_acc: 0.914696, train_avg_acc: 0.858306   test_loss: 0.309254, test_acc: 0.865000, test_avg_acc: 0.865000\n",
            "Epoch: 75 lr: 7.960e-02   train_loss: 0.190757, train_acc: 0.915541, train_avg_acc: 0.874231   test_loss: 0.267754, test_acc: 0.900000, test_avg_acc: 0.900000\n",
            "Epoch: 76 lr: 7.909e-02   train_loss: 0.168236, train_acc: 0.929899, train_avg_acc: 0.881674   test_loss: 0.292352, test_acc: 0.868333, test_avg_acc: 0.868333\n",
            "Epoch: 77 lr: 7.858e-02   train_loss: 0.162165, train_acc: 0.931588, train_avg_acc: 0.901787   test_loss: 0.244453, test_acc: 0.900000, test_avg_acc: 0.900000\n",
            "Epoch: 78 lr: 7.807e-02   train_loss: 0.174405, train_acc: 0.923142, train_avg_acc: 0.882099   test_loss: 0.444277, test_acc: 0.833333, test_avg_acc: 0.833333\n",
            "Epoch: 79 lr: 7.755e-02   train_loss: 0.162170, train_acc: 0.928209, train_avg_acc: 0.894191   test_loss: 0.277874, test_acc: 0.871667, test_avg_acc: 0.871667\n",
            "Epoch: 80 lr: 7.702e-02   train_loss: 0.160471, train_acc: 0.930743, train_avg_acc: 0.895091   test_loss: 0.242496, test_acc: 0.905000, test_avg_acc: 0.905000\n",
            "Epoch: 81 lr: 7.650e-02   train_loss: 0.132235, train_acc: 0.941723, train_avg_acc: 0.919756   test_loss: 0.371696, test_acc: 0.816667, test_avg_acc: 0.816667\n",
            "Epoch: 82 lr: 7.596e-02   train_loss: 0.234902, train_acc: 0.905405, train_avg_acc: 0.863692   test_loss: 0.325801, test_acc: 0.865000, test_avg_acc: 0.865000\n",
            "Epoch: 83 lr: 7.543e-02   train_loss: 0.210746, train_acc: 0.906250, train_avg_acc: 0.837007   test_loss: 0.345966, test_acc: 0.836667, test_avg_acc: 0.836667\n",
            "Epoch: 84 lr: 7.489e-02   train_loss: 0.185182, train_acc: 0.913851, train_avg_acc: 0.878828   test_loss: 0.286696, test_acc: 0.868333, test_avg_acc: 0.868333\n",
            "Epoch: 85 lr: 7.435e-02   train_loss: 0.173410, train_acc: 0.930743, train_avg_acc: 0.896284   test_loss: 0.274226, test_acc: 0.895000, test_avg_acc: 0.895000\n",
            "Epoch: 86 lr: 7.380e-02   train_loss: 0.157561, train_acc: 0.928209, train_avg_acc: 0.892083   test_loss: 0.380162, test_acc: 0.845000, test_avg_acc: 0.845000\n",
            "Epoch: 87 lr: 7.325e-02   train_loss: 0.181204, train_acc: 0.916385, train_avg_acc: 0.870720   test_loss: 0.294809, test_acc: 0.885000, test_avg_acc: 0.885000\n",
            "Epoch: 88 lr: 7.269e-02   train_loss: 0.157540, train_acc: 0.941723, train_avg_acc: 0.904737   test_loss: 0.246561, test_acc: 0.898333, test_avg_acc: 0.898333\n",
            "Epoch: 89 lr: 7.214e-02   train_loss: 0.160895, train_acc: 0.940034, train_avg_acc: 0.916255   test_loss: 0.266287, test_acc: 0.885000, test_avg_acc: 0.885000\n",
            "Epoch: 90 lr: 7.158e-02   train_loss: 0.182267, train_acc: 0.918074, train_avg_acc: 0.881752   test_loss: 0.433332, test_acc: 0.800000, test_avg_acc: 0.800000\n",
            "Epoch: 91 lr: 7.101e-02   train_loss: 0.274435, train_acc: 0.871622, train_avg_acc: 0.799386   test_loss: 0.474275, test_acc: 0.748333, test_avg_acc: 0.748333\n",
            "Epoch: 92 lr: 7.044e-02   train_loss: 0.236914, train_acc: 0.890203, train_avg_acc: 0.823073   test_loss: 0.322166, test_acc: 0.855000, test_avg_acc: 0.855000\n",
            "Epoch: 93 lr: 6.987e-02   train_loss: 0.213617, train_acc: 0.898649, train_avg_acc: 0.862576   test_loss: 0.387171, test_acc: 0.823333, test_avg_acc: 0.823333\n",
            "Epoch: 94 lr: 6.930e-02   train_loss: 0.188696, train_acc: 0.913007, train_avg_acc: 0.866988   test_loss: 0.288436, test_acc: 0.870000, test_avg_acc: 0.870000\n",
            "Epoch: 95 lr: 6.872e-02   train_loss: 0.188626, train_acc: 0.908784, train_avg_acc: 0.860220   test_loss: 0.273280, test_acc: 0.896667, test_avg_acc: 0.896667\n",
            "Epoch: 96 lr: 6.814e-02   train_loss: 0.198618, train_acc: 0.908784, train_avg_acc: 0.861226   test_loss: 0.255868, test_acc: 0.893333, test_avg_acc: 0.893333\n",
            "Epoch: 97 lr: 6.756e-02   train_loss: 0.160505, train_acc: 0.929054, train_avg_acc: 0.886248   test_loss: 0.305267, test_acc: 0.868333, test_avg_acc: 0.868333\n",
            "Epoch: 98 lr: 6.697e-02   train_loss: 0.157909, train_acc: 0.927365, train_avg_acc: 0.882041   test_loss: 0.255037, test_acc: 0.898333, test_avg_acc: 0.898333\n",
            "Epoch: 99 lr: 6.639e-02   train_loss: 0.150884, train_acc: 0.944257, train_avg_acc: 0.911231   test_loss: 0.280225, test_acc: 0.881667, test_avg_acc: 0.881667\n",
            "Epoch: 100 lr: 6.580e-02   train_loss: 0.165836, train_acc: 0.928209, train_avg_acc: 0.889621   test_loss: 0.266270, test_acc: 0.881667, test_avg_acc: 0.881667\n",
            "Epoch: 101 lr: 6.520e-02   train_loss: 0.157922, train_acc: 0.931588, train_avg_acc: 0.888444   test_loss: 0.219586, test_acc: 0.918333, test_avg_acc: 0.918333\n",
            "Epoch: 102 lr: 6.461e-02   train_loss: 0.120437, train_acc: 0.951858, train_avg_acc: 0.920400   test_loss: 0.204995, test_acc: 0.930000, test_avg_acc: 0.930000\n",
            "Epoch: 103 lr: 6.401e-02   train_loss: 0.123581, train_acc: 0.956926, train_avg_acc: 0.938292   test_loss: 0.251250, test_acc: 0.898333, test_avg_acc: 0.898333\n",
            "Epoch: 104 lr: 6.341e-02   train_loss: 0.142016, train_acc: 0.953547, train_avg_acc: 0.927995   test_loss: 0.205440, test_acc: 0.903333, test_avg_acc: 0.903333\n",
            "Epoch: 105 lr: 6.281e-02   train_loss: 0.148660, train_acc: 0.941723, train_avg_acc: 0.907184   test_loss: 0.276741, test_acc: 0.885000, test_avg_acc: 0.885000\n",
            "Epoch: 106 lr: 6.221e-02   train_loss: 0.156581, train_acc: 0.934966, train_avg_acc: 0.908576   test_loss: 0.205472, test_acc: 0.923333, test_avg_acc: 0.923333\n",
            "Epoch: 107 lr: 6.160e-02   train_loss: 0.118927, train_acc: 0.948480, train_avg_acc: 0.918231   test_loss: 0.156021, test_acc: 0.950000, test_avg_acc: 0.950000\n",
            "Epoch: 108 lr: 6.099e-02   train_loss: 0.095218, train_acc: 0.971284, train_avg_acc: 0.953668   test_loss: 0.137669, test_acc: 0.958333, test_avg_acc: 0.958333\n",
            "Epoch: 109 lr: 6.039e-02   train_loss: 0.120104, train_acc: 0.950169, train_avg_acc: 0.927784   test_loss: 0.149854, test_acc: 0.943333, test_avg_acc: 0.943333\n",
            "Epoch: 110 lr: 5.978e-02   train_loss: 0.099837, train_acc: 0.964527, train_avg_acc: 0.944754   test_loss: 0.139448, test_acc: 0.946667, test_avg_acc: 0.946667\n",
            "Epoch: 111 lr: 5.916e-02   train_loss: 0.122838, train_acc: 0.952703, train_avg_acc: 0.925232   test_loss: 0.129885, test_acc: 0.953333, test_avg_acc: 0.953333\n",
            "Epoch: 112 lr: 5.855e-02   train_loss: 0.105337, train_acc: 0.959459, train_avg_acc: 0.934274   test_loss: 0.110842, test_acc: 0.968333, test_avg_acc: 0.968333\n",
            "Epoch: 113 lr: 5.794e-02   train_loss: 0.097605, train_acc: 0.964527, train_avg_acc: 0.944617   test_loss: 0.179383, test_acc: 0.941667, test_avg_acc: 0.941667\n",
            "Epoch: 114 lr: 5.732e-02   train_loss: 0.128158, train_acc: 0.945946, train_avg_acc: 0.918616   test_loss: 0.132669, test_acc: 0.956667, test_avg_acc: 0.956667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8549ed2afa65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# test(config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f15d680483fd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mtrain_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mtrain_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czwf3qI9FBq2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHLJACYHOjU1"
      },
      "source": [
        "Заключение.\n",
        "Модель склонна к переобучению, причем возможно на количество точек (у каждой фигуры оно свое).\n",
        "Как минимум нужен датасет где количество точек в облаке не связано с формой этого облака.\n",
        "Можно попробовать добавлять аугментации (jitter, translation, rotation, rotation вроде и так в датасете присутствует)\n",
        "\n",
        "Были мысли свести задачу к двумерной, например как описано тут: https://arxiv.org/abs/2106.05304, но хотелось попробовать архитектуру DGCNN.\n",
        "\n",
        "Требование про единый стиль кода видел, но т.к. часть кода взята с гитхаба, и работа велась в Colab, было проблематично привести его в нормальный вид и разбить на модули.\n",
        "Обычно я не использую тетрадки для тренировки моделей, а запускаю тренировку в скрине. На модули разбил бы так как тут разбито на ячейки. Т.е. отдельно модуль с архитектурой модели, отдельно с train loop, отдельно с dataloader. Не стал делать это разбиение тут, поскольку было бы сильно неудобно вносить правки в код. Еще в реальном кейсе labels надо сохранять в модель при конвертировании в jit. Сам train loop тоже разить на функции, или вообще перенести на Catalyst.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AIcoX_WcMso"
      },
      "source": [
        "Convert to jit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt0WDwI6RZ3V",
        "outputId": "a1824b1b-b1c1-4aaa-bb5a-d06e03c0b26f"
      },
      "source": [
        "config = Config(\n",
        "  exp_name = 'exp',\n",
        "  batch_size = 32,\n",
        "  test_batch_size = 16,\n",
        "  epochs = 250,\n",
        "  use_sgd = True,\n",
        "  lr = 0.001,\n",
        "  momentum = 0.9,\n",
        "  scheduler = 'cos',\n",
        "  seed = 1,     \n",
        "  num_points = 1024,\n",
        "  dropout = 0.5,\n",
        "  emb_dims = 1024,\n",
        "  k = 20,\n",
        "  device = 'cpu',\n",
        "  checkpoints_path = '/content/drive/My Drive/test_task_2/checkpoints/',\n",
        ") \n",
        "\n",
        "torch.manual_seed(config.seed)    \n",
        "torch.cuda.manual_seed(config.seed)    \n",
        "\n",
        "# train(config)    \n",
        "\n",
        "# test(config)\n",
        "\n",
        "convert_to_jit(config)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGCNN_cls(\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (linear1): Linear(in_features=2048, out_features=512, bias=False)\n",
            "  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dp1): Dropout(p=0.5, inplace=False)\n",
            "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dp2): Dropout(p=0.5, inplace=False)\n",
            "  (linear3): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06_Mnob9RbOd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}